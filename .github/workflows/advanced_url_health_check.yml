name: Advanced URL Health Check

on:
  schedule:
    - cron: '0 */6 * * *'
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  advanced-url-check:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y curl parallel jq

    - name: Download M3U file from GitHub Raw
      run: |
        M3U_URL="https://raw.githubusercontent.com/SivaRaaM15/myStreamS/refs/heads/main/sivarenu.m3u"
        echo "Fetching M3U from: $M3U_URL"
        curl -fsSL "$M3U_URL" -o sivarenu.m3u || {
          echo "Failed to download M3U file!"
          exit 1
        }
        if [ ! -s "sivarenu.m3u" ]; then
          echo "Downloaded M3U is empty!"
          exit 1
        fi
        echo "M3U file downloaded successfully."

    - name: Advanced URL Health Check
      run: |
        set -euo pipefail

        echo "Starting Advanced URL Health Check..."
        echo "==============================================="

        mkdir -p results
        > report.jsonl
        > summary.log

        # Parse M3U: extract tvg-id and URL
        awk '
          /^#EXTINF/ {
            title = $0
            gsub(/.*tvg-id="/, "", title)
            gsub(/".*/, "", title)
            if (title == "") title = "Unknown"
          }
          /^https?:\/\// {
            gsub(/\r$/, "", $0)
            print title "|" $0
          }
        ' sivarenu.m3u > url_list.txt

        TOTAL_URLS=$(wc -l < url_list.txt)
        echo "Found $TOTAL_URLS streams to test."

        if [ "$TOTAL_URLS" -eq 0 ]; then
          echo "No URLs found in M3U!"
          exit 1
        fi

        # Test function — uses GET with range (no HEAD)
        test_url() {
          local line="$1"
          local num="$2"
          local title=$(echo "$line" | cut -d'|' -f1)
          local url=$(echo "$line" | cut -d'|' -f2- | sed 's/[[:space:]]*$//')

          local result_file="results/result_$num.json"
          local log_file="results/log_$num.txt"

          # Default values
          local http_code="000"
          local content_type="unknown"
          local response_time="0"
          local is_stream=0
          local has_ts=0
          local bitrate="unknown"
          local token_expiry="unknown"
          local status="failed"
          local debug=""

          # VLC-like headers
          local user_agent="VLC/3.0.0 LibVLC/3.0.0"
          local accept="*/*"
          local referer=""

          # Special handling
          if echo "$url" | grep -q "dailymotion.com"; then
            referer="https://www.dailymotion.com/"
          elif echo "$url" | grep -q "ciinema.net"; then
            referer="https://ciinema.net/"
          fi

          # Use GET with range to fetch first 2KB — avoids 405
          local response
          response=$(curl -s --max-time 20 \
            -r 0-2047 \
            -L \
            -A "$user_agent" \
            -H "Accept: $accept" \
            $( [ -n "$referer" ] && echo "-H 'Referer: $referer'" ) \
            --write-out "\nHTTP_CODE:%{http_code}\nCONTENT_TYPE:%{content_type}\nTIME_TOTAL:%{time_total}\nURL_EFFECTIVE:%{url_effective}" \
            "$url" 2>/dev/null || echo "HTTP_CODE:000\nCONTENT_TYPE:unknown\nTIME_TOTAL:20\nURL_EFFECTIVE:$url")

          http_code=$(echo "$response" | grep -m1 "HTTP_CODE:" | cut -d: -f2)
          content_type=$(echo "$response" | grep -m1 "CONTENT_TYPE:" | cut -d: -f2- | head -1)
          response_time=$(echo "$response" | grep -m1 "TIME_TOTAL:" | cut -d: -f2)
          local final_url=$(echo "$response" | grep -m1 "URL_EFFECTIVE:" | cut -d: -f2-)

          # Extract body
          local body=$(echo "$response" | sed -n '/^HTTP_CODE:/,$d' | head -c 2048)

          # Check for HLS
          if [[ "$http_code" == "200" || "$http_code" == "206" ]]; then
            if echo "$body" | grep -q "#EXTM3U"; then
              is_stream=1
            fi

            # Optional .ts check (skip on timeout)
            if [ $is_stream -eq 1 ]; then
              local base_url="${final_url%/*}"
              local ts_line=$(echo "$body" | grep -m1 "\.ts$" || true)
              if [ -n "$ts_line" ] && [[ "$ts_line" != http* ]]; then
                local ts_url="$base_url/$ts_line"
                if curl -s -I --max-time 10 "$ts_url" 2>/dev/null | grep -q "200"; then
                  has_ts=1
                fi
              fi

              # Bitrate
              bitrate=$(echo "$body" | grep -m1 "BANDWIDTH" | grep -o "BANDWIDTH=[0-9]*" | cut -d= -f2 || echo "unknown")
              [ "$bitrate" = "" ] && bitrate="unknown"
            fi
          fi

          # Token expiry
          if echo "$url" | grep -q "ciinema.net.*e=[0-9]*"; then
            local expiry_sec
            expiry_sec=$(echo "$url" | grep -o "e=[0-9]*" | cut -d= -f2)
            local now_sec
            now_sec=$(date +%s)
            local diff_sec=$(( expiry_sec - now_sec ))
            if [ $diff_sec -gt 0 ]; then
              local hours=$(( diff_sec / 3600 ))
              token_expiry="${hours}h"
            else
              token_expiry="expired"
            fi
          fi

          # Final status
          if [[ "$http_code" == "200" || "$http_code" == "206" ]]; then
            if [ $is_stream -eq 1 ] && [ $has_ts -eq 1 ]; then
              status="healthy"
            elif [ $is_stream -eq 1 ]; then
              status="warning"
            else
              status="caution"
            fi
          fi

          # Build JSON with jq
          jq -n \
            --arg idx "$num" \
            --arg t "$title" \
            --arg u "$url" \
            --arg fu "$final_url" \
            --arg hc "$http_code" \
            --arg ct "$content_type" \
            --arg rt "$response_time" \
            --argjson stream "$is_stream" \
            --argjson ts "$has_ts" \
            --arg br "$bitrate" \
            --arg te "$token_expiry" \
            --arg st "$status" \
            '{
              index: $idx,
              title: $t,
              url: $u,
              final_url: $fu,
              http_code: $hc,
              content_type: $ct,
              response_time: $rt,
              is_stream: $stream,
              has_ts_segment: $ts,
              bitrate: $br,
              token_expiry: $te,
              status: $st
            }' > "$result_file"

          # Human log
          {
            echo "[$num] $title"
            echo "   URL: ${url:0:60}..."
            echo "   Final: ${final_url:0:60}..."
            echo "   HTTP: $http_code | Type: $content_type"
            echo "   Time: ${response_time}s"
            echo "   Stream: $([ $is_stream -eq 1 ] && echo "Yes" || echo "No") | TS: $([ $has_ts -eq 1 ] && echo "Yes" || echo "No")"
            echo "   Status: $status"
            [ "$token_expiry" != "unknown" ] && echo "   Token: $token_expiry"
            echo
          } > "$log_file"
        }

        export -f test_url
        seq 1 $TOTAL_URLS | parallel -j 3 --halt now,fail=1 test_url

        # Combine results
        for f in results/result_*.json; do cat "$f"; echo; done > report.jsonl

        # Summary
        HEALTHY=$(jq 'select(.status == "healthy")' report.jsonl | wc -l)
        WARNING=$(jq 'select(.status == "warning")' report.jsonl | wc -l)
        CAUTION=$(jq 'select(.status == "caution")' report.jsonl | wc -l)
        FAILED=$(jq 'select(.status == "failed")' report.jsonl | wc -l)

        {
          echo "==============================================="
          echo "ADVANCED URL HEALTH CHECK SUMMARY"
          echo "==============================================="
          echo "Generated: $(date -u '+%Y-%m-%d %H:%M:%SZ')"
          echo "Total URLs: $TOTAL_URLS"
          echo "Healthy: $HEALTHY"
          echo "Warning: $WARNING"
          echo "Caution: $CAUTION"
          echo "Failed: $FAILED"
          echo "==============================================="
          echo
          echo "DETAILED LOGS:"
          echo "--------------"
          for f in results/log_*.txt; do cat "$f"; done
        } > health-summary.txt

        echo "Check completed."

        # Exit logic: only CRITICAL if ZERO healthy+warning
        if [ $((HEALTHY + WARNING)) -eq 0 ]; then
          echo "CRITICAL: No playable streams!"
          exit 1
        else
          echo "SUCCESS: At least one stream is playable."
          exit 0
        fi

    - name: Upload Health Report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: url-health-report
        path: |
          sivarenu.m3u
          health-summary.txt
          report.jsonl
          results/
        retention-days: 7
