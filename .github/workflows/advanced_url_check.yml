name: Advanced URL Health Check

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  advanced-url-check:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y curl parallel  # Use parallel for faster processing

    - name: Fast URL Health Check
      run: |
        echo "üöÄ Starting Fast URL Health Check..."
        echo "======================================"
        
        # Check if file exists
        if [ ! -f "sivarenu.m3u" ]; then
          echo "‚ùå M3U file not found!"
          exit 1
        fi

        # Extract URLs (limit to 8 for speed)
        grep "^http" sivarenu.m3u | head -8 > urls_to_test.txt
        TOTAL_URLS=$(wc -l < urls_to_test.txt)
        
        echo "üìä Testing $TOTAL_URLS URLs with parallel processing..."
        echo ""

        # Function to test a single URL
        test_single_url() {
          local URL="$1"
          local NUM="$2"
          
          echo "üîç Testing URL $NUM..."
          echo "   URL: ${URL:0:50}..."
          
          # Clean URL
          CLEAN_URL=$(echo "$URL" | tr -d '\r' | sed 's/[[:space:]]*$//')
          
          # SINGLE CURL CALL with multiple checks
          RESPONSE=$(curl -s -I -L --max-time 10 --user-agent "Mozilla/5.0" --write-out "HTTP_CODE:%{http_code}\nCONTENT_TYPE:%{content_type}\nREDIRECTS:%{num_redirects}\nTIME_TOTAL:%{time_total}\n" "$CLEAN_URL" 2>/dev/null || echo "HTTP_CODE:000")
          
          # Parse response
          HTTP_CODE=$(echo "$RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
          CONTENT_TYPE=$(echo "$RESPONSE" | grep "CONTENT_TYPE:" | cut -d: -f2-)
          REDIRECTS=$(echo "$RESPONSE" | grep "REDIRECTS:" | cut -d: -f2)
          RESPONSE_TIME=$(echo "$RESPONSE" | grep "TIME_TOTAL:" | cut -d: -f2)
          
          # Quick stream check (only first few bytes)
          IS_STREAM=0
          if curl -s --max-time 5 --range 0-1000 --user-agent "VLC" "$CLEAN_URL" 2>/dev/null | grep -q "EXTM3U\|#EXTINF"; then
            IS_STREAM=1
          fi
          
          # Display results
          echo "   üìä Status: HTTP $HTTP_CODE"
          echo "   üìÅ Type: ${CONTENT_TYPE:-unknown}"
          echo "   üîÑ Redirects: $REDIRECTS"
          echo "   ‚è±Ô∏è  Time: ${RESPONSE_TIME}s"
          echo "   üì∫ Stream: $([ $IS_STREAM -eq 1 ] && echo 'Yes' || echo 'No')"
          
          # Health assessment
          if [ "$HTTP_CODE" = "200" ] || [ "$HTTP_CODE" = "206" ]; then
            if [ $IS_STREAM -eq 1 ]; then
              echo "   ‚úÖ HEALTHY - Working stream"
              echo "HEALTHY" > "result_$NUM.txt"
            elif echo "$CONTENT_TYPE" | grep -q "video\|audio"; then
              echo "   ‚úÖ HEALTHY - Media content"
              echo "HEALTHY" > "result_$NUM.txt"
            else
              echo "   ‚ö†Ô∏è  CAUTION - Not a stream"
              echo "WARNING" > "result_$NUM.txt"
            fi
          else
            echo "   ‚ùå PROBLEM - HTTP $HTTP_CODE"
            echo "ERROR" > "result_$NUM.txt"
          fi
          
          echo ""
        }

        # Export function for parallel use
        export -f test_single_url

        # Process URLs in parallel (max 2 at a time)
        cat urls_to_test.txt | parallel -j 2 --line-buffer "test_single_url {} {#}"

        # Count results
        HEALTHY_COUNT=$(ls result_*.txt 2>/dev/null | xargs grep -l "HEALTHY" 2>/dev/null | wc -l || echo 0)
        ERROR_COUNT=$(ls result_*.txt 2>/dev/null | xargs grep -l "ERROR" 2>/dev/null | wc -l || echo 0)
        
        # Cleanup
        rm -f urls_to_test.txt result_*.txt 2>/dev/null || true

        # Final Summary
        echo "=========================================="
        echo "üìà HEALTH CHECK SUMMARY"
        echo "=========================================="
        echo "‚úÖ Healthy URLs: $HEALTHY_COUNT/$TOTAL_URLS"
        echo "‚ùå Problematic URLs: $ERROR_COUNT"
        echo "‚è±Ô∏è  Check completed at: $(date)"
        
        # Set workflow status
        if [ $HEALTHY_COUNT -eq 0 ]; then
          echo "‚ùå CRITICAL: No healthy URLs found!"
          exit 1
        elif [ $HEALTHY_COUNT -lt $((TOTAL_URLS / 2)) ]; then
          echo "‚ö†Ô∏è  WARNING: Less than 50% URLs are healthy"
          exit 0
        else
          echo "‚úÖ SUCCESS: Majority of URLs are healthy"
          exit 0
        fi

    - name: Upload Health Report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: url-health-report
        path: |
          sivarenu.m3u
        retention-days: 3
